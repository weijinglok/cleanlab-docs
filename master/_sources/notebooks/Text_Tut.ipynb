{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Cleanlab, TensorFlow, & SciKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will use Cleanlab to find potential label errors in the IMDb movie review dataset. This dataset contains 50,000 labeled text reviews split evenly in the train and test set. Each review is labeled with a binary sentiment polarity label - positive (1) or negative (0). Cleanlab will shortlist *hundreds* of examples that confuses our ML model the most; many of which are potential label errors, edge cases and obscure examples.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Build a simple TensorFlow & Keras neural net and wrap it with SciKeras to make it scikit-learn compatible.\n",
    "\n",
    "- Compute the out-of-sample predicted probabilities, ``pyx``, with cross validation.\n",
    "\n",
    "- Generate a list of potential label errors with Cleanlab's ``get_noise_indices``.\n",
    "\n",
    "- Build and train aa robust model with Cleanlab's ``LearningWithNoisyLabels`` wrapper. \n",
    "\n",
    "**Data:** https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Install the required dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``%%capture`` is a magic function to hide the cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install cleanlab sklearn pandas tensorflow tensorflow-datasets scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load the ACL's IMDb dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the full dataset from TensorFlow Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_full_ds = tfds.load(name='imdb_reviews', split=('train+test'), batch_size=-1, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into two numpy arrays:\n",
    "1. ``raw_full_texts`` for the movie reviews in text format, and\n",
    "2. ``full_labels`` for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_full_texts, full_labels = tfds.as_numpy(raw_full_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Preprocess the text data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that can standardize the text data in three steps:\n",
    "1. Convert it to lower case\n",
    "2. Remove the HTML break tags, ``<br />``\n",
    "3. Remove any punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html, f'[{re.escape(string.punctuation)}]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ``TextVectorization`` layer that can standardize (by running the ``custom_standardization`` function we've just defined above), tokenize and vectorize our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting ``vectorize_layer`` on our text data creates a mapping of each token to an integer. After that, we can vectorize our text data with the adapted ``vectorize_layer``. Finally, we'll also convert our text data into a numpy array as required by Cleanlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(raw_full_texts)\n",
    "\n",
    "full_texts = vectorize_layer(raw_full_texts)\n",
    "\n",
    "full_texts = full_texts.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Build a classifcation model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build and compile a simple neural network with TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
    "  layers.Embedding(max_features + 1, 16),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Wrap with SciKeras for scikit-learn compatibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of Cleanlab's feature requires scikit-learn compatibility, we will need to adapt the above TensorFlow & Keras neural net accordingly. SciKeras is a convenient package that helps with this, read more about it here: https://www.adriangb.com/scikeras/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(model, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Compute the out-of-sample predicted probabilities with cross validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the entire dataset on the model used to compute the out-of-sample predicted probabilities, ``pyx``, with cross validation. This model will not be used for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(full_texts, full_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the out-of-sample predicted probabilities, ``pyx``, with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "pyx = cross_val_predict(model, full_texts, full_labels, cv=3, method='predict_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Run Cleanlab's to find potential label errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanlab has a ``get_noise_indices`` function to generate a list of potential label errors. Setting ``sorted_index_method=\"prob_given_label\"`` returns the indices of all the most likely label errors, sorted by the most suspicious example first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.pruning import get_noise_indices\n",
    "\n",
    "ordered_label_errors = get_noise_indices(\n",
    "    s=full_labels,\n",
    "    psx=pyx,\n",
    "    sorted_index_method=\"prob_given_label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Review some of the highest potential label errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cleanlab found {len(ordered_label_errors)} potential label errors. Here are the indices of the top 10 most likely ones: \\n {ordered_label_errors[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change pandas display max column width to ``None`` and define a new function, ``print_as_df``, that can print any example from the raw dataset with just its index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def print_as_df(index):\n",
    "    return pd.DataFrame({'texts': raw_full_texts[index], 'labels': full_labels[index]}, [index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review labeled as positive (1), but should be negative (0). Here are some review snippets:\n",
    "\n",
    "> - \"...incredibly **awful** score...\"\n",
    ">\n",
    "> - \"...**worst** Foley work ever done.\"\n",
    ">\n",
    "> - \"...script is **incomprehensible**...\"\n",
    ">\n",
    "> - \"...editing is just **bizarre**.\"\n",
    ">\n",
    "> - \"...**atrocious** pan and scan...\"\n",
    ">\n",
    "> - \"...**incoherent mess**...\"\n",
    ">\n",
    "> - \"...**amateur** directing there.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_as_df(44582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review labeled as positive (1), but should be negative (0). Here are some review snippets:\n",
    "\n",
    "> - \"...film seems **cheap**.\"\n",
    ">\n",
    "> - \"...unbelievably **bad**...\"\n",
    ">\n",
    "> - \"...cinematography is **badly** lit...\"\n",
    ">\n",
    "> - \"...everything looking **grainy** and **ugly**.\"\n",
    ">\n",
    "> - \"...sound is so **terrible**...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_as_df(10404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review labeled as positive (1), but should be negative (0). Here are some review snippets:\n",
    "\n",
    "> - \"...hard to imagine a **boring** shark movie...\"\n",
    ">\n",
    "> - \"**Poor focus** in some scenes made the production seems **amateurish**.\"\n",
    ">\n",
    "> - \"...**do nothing** to take advantage of...\"\n",
    ">\n",
    "> - \"...**far too few** scenes of any depth or variety.\"\n",
    ">\n",
    "> - \"...just **look flat**...no contrast of depth...\"\n",
    ">\n",
    "> - \"...**introspective** and **dull**...constant **disappointment**.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_as_df(30151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanlab has shortlisted the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix label errors, augment edge cases or remove obscure examples. \n",
    "\n",
    "These human-in-the-loop processes may be time-consuming, so if you'd like Cleanlab to automatically remove these noisy examples and train a model directly on the partially mislabeled dataset, you're in luck! Cleanlab provides a ``LearningWithNoisyLabels`` wrapper to do precisely this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Adapt with Cleanlab's wrapper and train a robust model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we will load the dataset, but this time, we will load the train and test set separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = tfds.load(name='imdb_reviews', split='train', batch_size=-1, as_supervised=True)\n",
    "raw_test_ds = tfds.load(name='imdb_reviews', split='test', batch_size=-1, as_supervised=True)\n",
    "\n",
    "raw_train_texts, train_labels = tfds.as_numpy(raw_train_ds)\n",
    "raw_test_texts, test_labels = tfds.as_numpy(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same ``vectorize_layer`` as before, but we will reset its state and adapt it only on the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.reset_state() \n",
    "\n",
    "vectorize_layer.adapt(raw_train_texts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the text data in the train and test sets, then convert them into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = vectorize_layer(raw_train_texts) \n",
    "test_texts = vectorize_layer(raw_test_texts)\n",
    "\n",
    "train_texts = train_texts.numpy()\n",
    "test_texts = test_texts.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ``clone`` function to construct a new unfitted model then wrap it with Cleanlab's ``LearningWithNoisyLabels`` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "\n",
    "model = clone(model)\n",
    "\n",
    "lnl = LearningWithNoisyLabels(clf=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the wrapped model, ``lnl``, on the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lnl.fit(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Evaluate the robust model's performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred_labels = lnl.predict(test_texts)\n",
    "accuracy_score(test_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What's next?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation on completing this tutorial! Check out our following tutorial on using Cleanlab for tabular data classification!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text x TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
