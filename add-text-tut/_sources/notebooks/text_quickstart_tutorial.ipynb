{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with TensorFlow, Keras, and Cleanlab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quick-start tutorial, we use `cleanlab` to find potential label errors in the IMDb movie review text classification dataset. This dataset contains 50,000 text reviews, each labeled with a binary sentiment polarity label indicating whether the review is positive (1) or negative (0). `cleanlab` will shortlist _hundreds_ of examples that confuse our ML model the most; many of which are potential label errors, edge cases, or otherwise ambiguous examples.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Build a simple TensorFlow & Keras neural net and wrap it with SciKeras to make it scikit-learn compatible.\n",
    "\n",
    "- Use this classifier to compute out-of-sample predicted probabilities, `pred_probs`, via cross validation.\n",
    "\n",
    "- Identify potential label errors in the data with `cleanlab`'s `find_label_issues` method.\n",
    "\n",
    "- Train a more robust version of the same neural net via `cleanlab`'s `LearningWithNoisyLabels` wrapper.\n",
    "\n",
    "**Data:** https://ai.stanford.edu/~amaas/data/sentiment/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Install required dependencies**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following dependencies with `pip install`:\n",
    "\n",
    "1. cleanlab\n",
    "2. sklearn\n",
    "3. pandas\n",
    "4. tensorflow\n",
    "5. tensorflow-datasets\n",
    "6. scikeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "dependencies = [\"cleanlab\", \"sklearn\", \"pandas\", \"tensorflow\", \"tensorflow_datasets\", \"scikeras\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install git+https://github.com/weijinglok/cleanlab.git@be301eb3da59e32e2bb2d7efbb3a689ce17ded56\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # controls amount of tensorflow output\n",
    "SEED = 123  # just for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load and preprocess the IMDb text dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is provided in TensorFlow's Datasets. We will print the first example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Label: 0\n",
      "Example Text: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_full_ds = tfds.load(\n",
    "    name=\"imdb_reviews\", split=(\"train+test\"), batch_size=-1, as_supervised=True\n",
    ")\n",
    "raw_full_texts, full_labels = tfds.as_numpy(raw_full_ds)\n",
    "\n",
    "i = 0\n",
    "print(f\"Example Label: {full_labels[i]}\")\n",
    "print(f\"Example Text: {raw_full_texts[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored as two numpy arrays:\n",
    "\n",
    "1. `raw_full_texts` for the movie reviews in text format,\n",
    "2. `full_labels` for the labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bringing Your Own Data (BYOD)?\n",
    "\n",
    "You can easily replace the above with your own tabular dataset, and continue with the rest of the tutorial.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to preprocess the text data by:\n",
    "\n",
    "1. Converting it to lower case\n",
    "2. Removing the HTML break tags: `<br />`\n",
    "3. Removing any punctuation marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def preprocess_text(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `TextVectorization` layer to preprocess, tokenize, and vectorize our text data, thus making it suitable as input for a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=preprocess_text,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting `vectorize_layer` to the text data creates a mapping of each token (i.e. word) to an integer index. Subsequently, we can vectorize our text data by using this mapping. Finally, we'll also convert our text data into a numpy array as required by `cleanlab`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(raw_full_texts)\n",
    "full_texts = vectorize_layer(raw_full_texts)\n",
    "full_texts = full_texts.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Define a classification model and compute out-of-sample predicted probabilities**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build a simple neural network for classification with TensorFlow and Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "\n",
    "\n",
    "def get_net():\n",
    "    net = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
    "            layers.Embedding(max_features + 1, 16),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(1),\n",
    "        ]\n",
    "    )  # outputs probability that text belongs to class 1\n",
    "\n",
    "    net.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=metrics.BinaryAccuracy(),\n",
    "    )\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of `cleanlab`'s feature requires scikit-learn compatibility, we will need to adapt the above TensorFlow & Keras neural net accordingly. [SciKeras](https://www.adriangb.com/scikeras/stable/) is a convenient package that makes this really easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(get_net(), epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify label issues, cleanlab requires a probabilistic prediction from your model for every datapoint that should be considered. However these predictions will be _overfit_ (and thus unreliable) for datapoints the model was previously trained on. `cleanlab` is intended to only be used with **out-of-sample** predicted probabilities, i.e. on datapoints held-out from the model during the training.\n",
    "\n",
    "K-fold cross-validation is a straightforward way to produce out-of-sample predicted probabilites for every datapoint in the dataset, by training K copies of our model on different data subsets and using each copy to predict on the subset of data it did not see during training. We can obtain cross-validated out-of-sample predicted probabilities from any classifier via a scikit-learn simple wrapper:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmptyags7k2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmptyags7k2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1042/1042 [==============================] - 14s 12ms/step - loss: 0.6264 - binary_accuracy: 0.5510\n",
      "Epoch 2/10\n",
      "1042/1042 [==============================] - 15s 15ms/step - loss: 0.4563 - binary_accuracy: 0.7820\n",
      "Epoch 3/10\n",
      "1042/1042 [==============================] - 15s 15ms/step - loss: 0.3628 - binary_accuracy: 0.8447\n",
      "Epoch 4/10\n",
      "1042/1042 [==============================] - 15s 14ms/step - loss: 0.3144 - binary_accuracy: 0.8692\n",
      "Epoch 5/10\n",
      "1042/1042 [==============================] - 15s 15ms/step - loss: 0.2837 - binary_accuracy: 0.8820\n",
      "Epoch 6/10\n",
      "1042/1042 [==============================] - 14s 14ms/step - loss: 0.2629 - binary_accuracy: 0.8911\n",
      "Epoch 7/10\n",
      "1042/1042 [==============================] - 17s 16ms/step - loss: 0.2461 - binary_accuracy: 0.8988\n",
      "Epoch 8/10\n",
      "1042/1042 [==============================] - 14s 13ms/step - loss: 0.2319 - binary_accuracy: 0.9058\n",
      "Epoch 9/10\n",
      "1042/1042 [==============================] - 12s 12ms/step - loss: 0.2207 - binary_accuracy: 0.9119\n",
      "Epoch 10/10\n",
      "1042/1042 [==============================] - 15s 14ms/step - loss: 0.2102 - binary_accuracy: 0.9154\n",
      "521/521 [==============================] - 2s 3ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpe_rzklh3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpe_rzklh3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1042/1042 [==============================] - 17s 15ms/step - loss: 0.6270 - binary_accuracy: 0.5529\n",
      "Epoch 2/10\n",
      "1042/1042 [==============================] - 16s 15ms/step - loss: 0.4567 - binary_accuracy: 0.7815\n",
      "Epoch 3/10\n",
      "1042/1042 [==============================] - 15s 15ms/step - loss: 0.3625 - binary_accuracy: 0.8450\n",
      "Epoch 4/10\n",
      "1042/1042 [==============================] - 14s 14ms/step - loss: 0.3149 - binary_accuracy: 0.8689\n",
      "Epoch 5/10\n",
      "1042/1042 [==============================] - 16s 15ms/step - loss: 0.2852 - binary_accuracy: 0.8832\n",
      "Epoch 6/10\n",
      "1042/1042 [==============================] - 17s 16ms/step - loss: 0.2637 - binary_accuracy: 0.8930\n",
      "Epoch 7/10\n",
      "1042/1042 [==============================] - 16s 16ms/step - loss: 0.2454 - binary_accuracy: 0.9006\n",
      "Epoch 8/10\n",
      "1042/1042 [==============================] - 17s 16ms/step - loss: 0.2315 - binary_accuracy: 0.9074\n",
      "Epoch 9/10\n",
      "1042/1042 [==============================] - 16s 15ms/step - loss: 0.2201 - binary_accuracy: 0.9122\n",
      "Epoch 10/10\n",
      "1042/1042 [==============================] - 16s 15ms/step - loss: 0.2105 - binary_accuracy: 0.9156\n",
      "521/521 [==============================] - 2s 4ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpfck073_p\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpfck073_p\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1042/1042 [==============================] - 18s 16ms/step - loss: 0.6262 - binary_accuracy: 0.5583\n",
      "Epoch 2/10\n",
      "1042/1042 [==============================] - 19s 18ms/step - loss: 0.4530 - binary_accuracy: 0.7809\n",
      "Epoch 3/10\n",
      "1042/1042 [==============================] - 19s 18ms/step - loss: 0.3595 - binary_accuracy: 0.8451\n",
      "Epoch 4/10\n",
      "1042/1042 [==============================] - 18s 17ms/step - loss: 0.3127 - binary_accuracy: 0.8682\n",
      "Epoch 5/10\n",
      "1042/1042 [==============================] - 18s 17ms/step - loss: 0.2825 - binary_accuracy: 0.8822\n",
      "Epoch 6/10\n",
      "1042/1042 [==============================] - 18s 18ms/step - loss: 0.2603 - binary_accuracy: 0.8940\n",
      "Epoch 7/10\n",
      "1042/1042 [==============================] - 19s 18ms/step - loss: 0.2437 - binary_accuracy: 0.9009\n",
      "Epoch 8/10\n",
      "1042/1042 [==============================] - 18s 17ms/step - loss: 0.2294 - binary_accuracy: 0.9070\n",
      "Epoch 9/10\n",
      "1042/1042 [==============================] - 18s 17ms/step - loss: 0.2176 - binary_accuracy: 0.9124\n",
      "Epoch 10/10\n",
      "1042/1042 [==============================] - 18s 17ms/step - loss: 0.2084 - binary_accuracy: 0.9168\n",
      "521/521 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "num_crossval_folds = 3  # chosen for efficiency here, values like 5 or 10 will generally work better\n",
    "pred_probs = cross_val_predict(\n",
    "    model, full_texts, full_labels, cv=num_crossval_folds, method=\"predict_proba\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional benefit of cross-validation is that it facilitates more reliable evaluation of our model than a single training/validation split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated estimate of held-out AUC score: 0.9523431856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(full_labels, pred_probs[:, 1])\n",
    "print(f\"Cross-validated estimate of held-out AUC score: {auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Use cleanlab to find potential label errors**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given labels and out-of-sample predicted probabilities, `cleanlab` can quickly help us identify label issues. Here we request that the indices of the identified label issues should be sorted by `cleanlab`'s self-confidence score, which measures the quality of each given label via the probability assigned it in our model's prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.filter import find_label_issues\n",
    "\n",
    "ranked_label_issues = find_label_issues(\n",
    "    labels=full_labels, pred_probs=pred_probs, return_indices_ranked_by=\"self_confidence\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review some of the most likely label errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanlab found 1021 potential label errors. Here are indices of the top 10 most likely errors: \n",
      " [10404 44582 43777 30151 16633 21348 17701   434 29182  9634]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"cleanlab found {len(ranked_label_issues)} potential label errors. Here are indices of the top 10 most likely errors: \\n {ranked_label_issues[:10]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us inspect these datapoints, we define a method to print any example from the dataset. We then display some of the top-ranked label issues identified by `cleanlab`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "def print_as_df(index):\n",
    "    return pd.DataFrame({\"texts\": raw_full_texts[index], \"labels\": full_labels[index]}, [index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...incredibly **awful** score...\"\n",
    ">\n",
    "> - \"...**worst** Foley work ever done.\"\n",
    ">\n",
    "> - \"...script is **incomprehensible**...\"\n",
    ">\n",
    "> - \"...editing is just **bizarre**.\"\n",
    ">\n",
    "> - \"...**atrocious** pan and scan...\"\n",
    ">\n",
    "> - \"...**incoherent mess**...\"\n",
    ">\n",
    "> - \"...**amateur** directing there.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44582</th>\n",
       "      <td>b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.&lt;br /&gt;&lt;br /&gt;The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.&lt;br /&gt;&lt;br /&gt;The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.&lt;br /&gt;&lt;br /&gt;The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) &lt;br /&gt;&lt;br /&gt;This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.&lt;br /&gt;&lt;br /&gt;Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.&lt;br /&gt;&lt;br /&gt;Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            texts  \\\n",
       "44582  b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.<br /><br />The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.<br /><br />The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.<br /><br />The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) <br /><br />This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.<br /><br />Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.<br /><br />Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'   \n",
       "\n",
       "       labels  \n",
       "44582       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(44582)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...film seems **cheap**.\"\n",
    ">\n",
    "> - \"...unbelievably **bad**...\"\n",
    ">\n",
    "> - \"...cinematography is **badly** lit...\"\n",
    ">\n",
    "> - \"...everything looking **grainy** and **ugly**.\"\n",
    ">\n",
    "> - \"...sound is so **terrible**...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10404</th>\n",
       "      <td>b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    texts  \\\n",
       "10404  b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'   \n",
       "\n",
       "       labels  \n",
       "10404       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(10404)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review labeled as positive (1), but it should be negative (0).\n",
    "Some noteworthy snippets extracted from the review text:\n",
    "\n",
    "> - \"...hard to imagine a **boring** shark movie...\"\n",
    ">\n",
    "> - \"**Poor focus** in some scenes made the production seems **amateurish**.\"\n",
    ">\n",
    "> - \"...**do nothing** to take advantage of...\"\n",
    ">\n",
    "> - \"...**far too few** scenes of any depth or variety.\"\n",
    ">\n",
    "> - \"...just **look flat**...no contrast of depth...\"\n",
    ">\n",
    "> - \"...**introspective** and **dull**...constant **disappointment**.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30151</th>\n",
       "      <td>b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      texts  \\\n",
       "30151  b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'   \n",
       "\n",
       "       labels  \n",
       "30151       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(30151)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cleanlab` has shortlisted the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix these label issues or remove ambiguous examples from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Train a more robust model from noisy labels**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the label issues manually may be time-consuming, but at least `cleanlab` can filter these noisy examples and train a model on the remaining clean data for you automatically.\n",
    "To demonstrate this, we first reload the dataset, this time with separate train and test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = tfds.load(name=\"imdb_reviews\", split=\"train\", batch_size=-1, as_supervised=True)\n",
    "raw_test_ds = tfds.load(name=\"imdb_reviews\", split=\"test\", batch_size=-1, as_supervised=True)\n",
    "\n",
    "raw_train_texts, train_labels = tfds.as_numpy(raw_train_ds)\n",
    "raw_test_texts, test_labels = tfds.as_numpy(raw_test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We featurize the raw text using the same `vectorize_layer` as before, but first, reset its state and adapt it only on the train set (as is proper ML practice). We finally convert the vectorized text data in the train/test sets into numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.reset_state()\n",
    "vectorize_layer.adapt(raw_train_texts)\n",
    "\n",
    "train_texts = vectorize_layer(raw_train_texts)\n",
    "test_texts = vectorize_layer(raw_test_texts)\n",
    "\n",
    "train_texts = train_texts.numpy()\n",
    "test_texts = test_texts.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train and evaluate our original neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6490 - binary_accuracy: 0.5225\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5055 - binary_accuracy: 0.7249\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4022 - binary_accuracy: 0.8233\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.3452 - binary_accuracy: 0.8564\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.3091 - binary_accuracy: 0.8726\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.2820 - binary_accuracy: 0.8852\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.2605 - binary_accuracy: 0.8953\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.2440 - binary_accuracy: 0.9023\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.2293 - binary_accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.2177 - binary_accuracy: 0.9157\n",
      "782/782 [==============================] - 3s 4ms/step\n",
      "\n",
      " Test acuracy of original neural net: 0.86072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = KerasClassifier(get_net(), epochs=10)\n",
    "model.fit(train_texts, train_labels)\n",
    "\n",
    "preds = model.predict(test_texts)\n",
    "acc = accuracy_score(test_labels, preds)\n",
    "print(f\"\\n Test acuracy of original neural net: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cleanlab` provides a wrapper class that can easily be applied to any scikit-learn compatible model. Once wrapped, the resulting model can still be used in the exact same manner, but it will now train more robustly if the data have noisy labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "\n",
    "model = KerasClassifier(get_net(), epochs=10)  # Note we first re-instantiate the model\n",
    "lnl = LearningWithNoisyLabels(clf=model, seed=SEED)  # lnl has same methods/attributes as model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the `cleanlab`-wrapped model, the following operations take place: The original model is trained in a cross-validated fashion to produce out-of-sample predicted probabilities. Then, these predicted probabilites are used to identify label issues, which are then removed from the dataset. Finally, the original model is trained once more on the remaining clean subset of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpjp_yj3qx\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpjp_yj3qx\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 12ms/step - loss: 0.6626 - binary_accuracy: 0.5064\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.5475 - binary_accuracy: 0.6671\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.4443 - binary_accuracy: 0.7950\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3784 - binary_accuracy: 0.8388\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3355 - binary_accuracy: 0.8624\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3059 - binary_accuracy: 0.8750\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2822 - binary_accuracy: 0.8866\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2613 - binary_accuracy: 0.8960\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2456 - binary_accuracy: 0.9026\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2313 - binary_accuracy: 0.9103\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpjn6eegwf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpjn6eegwf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.6619 - binary_accuracy: 0.5074\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.5451 - binary_accuracy: 0.6715\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.4414 - binary_accuracy: 0.7951\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3747 - binary_accuracy: 0.8408\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3325 - binary_accuracy: 0.8611\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3022 - binary_accuracy: 0.8767\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2789 - binary_accuracy: 0.8881\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2597 - binary_accuracy: 0.8952\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2435 - binary_accuracy: 0.9039\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2297 - binary_accuracy: 0.9093\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpihgad0z4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpihgad0z4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.6624 - binary_accuracy: 0.5064\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.5459 - binary_accuracy: 0.6656\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4420 - binary_accuracy: 0.7966\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3763 - binary_accuracy: 0.8425\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3344 - binary_accuracy: 0.8630\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3039 - binary_accuracy: 0.8762\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2801 - binary_accuracy: 0.8881\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2615 - binary_accuracy: 0.8957\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2440 - binary_accuracy: 0.9032\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.2312 - binary_accuracy: 0.9098\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmprc0dx7x4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmprc0dx7x4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.6621 - binary_accuracy: 0.5047\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.5457 - binary_accuracy: 0.6686\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4425 - binary_accuracy: 0.7937\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3763 - binary_accuracy: 0.8401\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3332 - binary_accuracy: 0.8633\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3026 - binary_accuracy: 0.8763\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.2787 - binary_accuracy: 0.8884\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2579 - binary_accuracy: 0.8982\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2427 - binary_accuracy: 0.9064\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2283 - binary_accuracy: 0.9103\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpsm4pn192\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\lok_w\\AppData\\Local\\Temp\\tmpsm4pn192\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.6633 - binary_accuracy: 0.5053\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.5488 - binary_accuracy: 0.6605\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.4446 - binary_accuracy: 0.7955\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.3775 - binary_accuracy: 0.8407\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.3351 - binary_accuracy: 0.8622\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.3040 - binary_accuracy: 0.8762\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.2797 - binary_accuracy: 0.8872\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.2610 - binary_accuracy: 0.8939\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.2445 - binary_accuracy: 0.9006\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.2301 - binary_accuracy: 0.9092\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "762/762 [==============================] - 13s 15ms/step - loss: 0.6607 - binary_accuracy: 0.5263\n",
      "Epoch 2/10\n",
      "762/762 [==============================] - 13s 17ms/step - loss: 0.4995 - binary_accuracy: 0.7481\n",
      "Epoch 3/10\n",
      "762/762 [==============================] - 15s 20ms/step - loss: 0.3784 - binary_accuracy: 0.8471\n",
      "Epoch 4/10\n",
      "762/762 [==============================] - 14s 18ms/step - loss: 0.3081 - binary_accuracy: 0.8788\n",
      "Epoch 5/10\n",
      "762/762 [==============================] - 14s 18ms/step - loss: 0.2630 - binary_accuracy: 0.8956\n",
      "Epoch 6/10\n",
      "762/762 [==============================] - 15s 19ms/step - loss: 0.2313 - binary_accuracy: 0.9078\n",
      "Epoch 7/10\n",
      "762/762 [==============================] - 15s 19ms/step - loss: 0.2063 - binary_accuracy: 0.9158\n",
      "Epoch 8/10\n",
      "762/762 [==============================] - 15s 20ms/step - loss: 0.1867 - binary_accuracy: 0.9245\n",
      "Epoch 9/10\n",
      "762/762 [==============================] - 13s 17ms/step - loss: 0.1696 - binary_accuracy: 0.9313\n",
      "Epoch 10/10\n",
      "762/762 [==============================] - 14s 18ms/step - loss: 0.1554 - binary_accuracy: 0.9380\n"
     ]
    }
   ],
   "source": [
    "_ = lnl.fit(train_texts, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get predictions from the resulting cleanlab model and evaluate them, just like we did for our original neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step\n",
      "Test acuracy of cleanlab's neural net: 0.86852\n"
     ]
    }
   ],
   "source": [
    "pred_labels = lnl.predict(test_texts)\n",
    "acc = accuracy_score(test_labels, pred_labels)\n",
    "print(f\"Test acuracy of cleanlab's neural net: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the test set accuracy slightly improved as a result of the data cleaning. Note that this will not always be the case, especially when we are evaluating on test data that are themselves noisy. The best practice is to run `cleanlab` to identify potential label issues and then manually review them, before blindly trusting any accuracy metrics. In particular, the most effort should be made to ensure high-quality test data, which is supposed to reflect the expected performance of our model during deployment.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text x TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
