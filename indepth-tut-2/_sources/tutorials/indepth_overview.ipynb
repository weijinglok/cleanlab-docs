{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfmml1VCqCHm"
   },
   "source": [
    "# **Data-centric AI: Workflows with cleanlab 2.0+**\n",
    "\n",
    "In this tutorial, we demonstrate how you can easily incorporate the new and improved [cleanlab 2.0](https://github.com/cleanlab/cleanlab) into your ML development workflows to:\n",
    "\n",
    "- Automatically find label issues lurking in your data.\n",
    "- Score the label quality of every example in your dataset.\n",
    "- Train robust models in the presence of label issues.\n",
    "- Identify overlapping classes that you can merge to make the learning task less ambiguous.\n",
    "- Generate an overall label health score to track improvements in your labels as you clean your datasets over time.\n",
    "\n",
    "This tutorial provides an in-depth survey of many possible different ways that cleanlab can be utilized for Data-Centric AI. If you have a different use-case in mind that is not supported, please [tell us about it](https://github.com/cleanlab/cleanlab/issues)!\n",
    "\n",
    "Other resources to learn cleanlab: [Documentation](https://docs.cleanlab.ai/) | [\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBK4cAOUyLgW"
   },
   "source": [
    "## Install dependencies and import them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `pip` to install all packages required for this tutorial as follows:\n",
    "\n",
    "```ipython3\n",
    "!pip install sklearn matplotlib\n",
    "!pip install cleanlab\n",
    "# Make sure to install the version corresponding to this tutorial\n",
    "# E.g. if viewing master branch documentation:\n",
    "#     !pip install git+https://github.com/cleanlab/cleanlab.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "dependencies = [\"cleanlab\", \"sklearn\", \"matplotlib\"]\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):  # Check if it's running in Google Colab\n",
    "    %pip install git+https://github.com/weijinglok/cleanlab.git@7438acfe735dcea36533aa824ddd55d41a30fa9c\n",
    "    cmd = ' '.join([dep for dep in dependencies if dep != \"cleanlab\"])\n",
    "    %pip install $cmd\n",
    "else:\n",
    "    missing_dependencies = []\n",
    "    for dependency in dependencies:\n",
    "        try:\n",
    "            __import__(dependency)\n",
    "        except ImportError:\n",
    "            missing_dependencies.append(dependency)\n",
    "\n",
    "    if len(missing_dependencies) > 0:\n",
    "        print(\"Missing required dependencies:\")\n",
    "        print(*missing_dependencies, sep=\", \")\n",
    "        print(\"\\nPlease install them before running the rest of this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avXlHJcXjruP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cleanlab\n",
    "from cleanlab.classification import CleanLearning\n",
    "from cleanlab.benchmarking import noise_generation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy.random import multivariate_normal\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6VuupksjruQ"
   },
   "source": [
    "## Create the data used for this tutorial (can skip these details)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!-- This cell is for the collapsible block in the doc site -->\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"1\">Click here to view its code.</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "g2sOIFxbjruR",
    "outputId": "98977c55-0843-461c-93a7-3e6cc11ac518"
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "\n",
    "def make_data(\n",
    "    means=[[3, 2], [7, 7], [0, 8], [0, 10]],\n",
    "    covs=[\n",
    "        [[5, -1.5], [-1.5, 1]],\n",
    "        [[1, 0.5], [0.5, 4]],\n",
    "        [[5, 1], [1, 5]],\n",
    "        [[3, 1], [1, 1]],\n",
    "    ],\n",
    "    sizes=[100, 50, 50, 50],\n",
    "    avg_trace=0.8,\n",
    "    seed=SEED,  # set to None for non-reproducible randomness\n",
    "):\n",
    "    np.random.seed(seed=SEED)\n",
    "\n",
    "    K = len(means)  # number of classes\n",
    "    data = []\n",
    "    labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    for idx in range(K):\n",
    "        data.append(\n",
    "            np.random.multivariate_normal(\n",
    "                mean=means[idx], cov=covs[idx], size=sizes[idx]\n",
    "            )\n",
    "        )\n",
    "        test_data.append(\n",
    "            np.random.multivariate_normal(\n",
    "                mean=means[idx], cov=covs[idx], size=sizes[idx]\n",
    "            )\n",
    "        )\n",
    "        labels.append(np.array([idx for i in range(sizes[idx])]))\n",
    "        test_labels.append(np.array([idx for i in range(sizes[idx])]))\n",
    "    X_train = np.vstack(data)\n",
    "    y_train = np.hstack(labels)\n",
    "    X_test = np.vstack(test_data)\n",
    "    y_test = np.hstack(test_labels)\n",
    "\n",
    "    # Compute p(y=k)\n",
    "    py_true = np.bincount(y_train) / float(\n",
    "        len(y_train)\n",
    "    )  # Prior distribution over true labels.\n",
    "\n",
    "    noise_matrix_true = noise_generation.generate_noise_matrix_from_trace(\n",
    "        K,\n",
    "        trace=avg_trace * K,\n",
    "        py=py_true,\n",
    "        valid_noise_matrix=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Generate our noisy labels using the noise_marix.\n",
    "    s = noise_generation.generate_noisy_labels(y_train, noise_matrix_true)\n",
    "    s_test = noise_generation.generate_noisy_labels(y_test, noise_matrix_true)\n",
    "    ps = np.bincount(s) / float(len(s))  # Prior distribution over noisy labels\n",
    "\n",
    "    return {\n",
    "        \"data\": X_train,\n",
    "        \"true_labels\": y_train,  # You never get to see these perfect labels.\n",
    "        \"labels\": s,  # Instead, you have these labels, which have some errors.\n",
    "        \"test_data\": X_test,\n",
    "        \"test_labels\": y_test,  # Perfect labels used for \"true\" measure of model's performance during deployment.\n",
    "        \"noisy_test_labels\": s_test,  # With IID train/test split, you'd have these labels, which also have some errors.\n",
    "        \"ps\": ps,\n",
    "        \"py_true\": py_true,\n",
    "        \"noise_matrix_true\": noise_matrix_true,\n",
    "        \"class_names\": [\"purple\", \"blue\", \"seafoam green\", \"yellow\"],\n",
    "    }\n",
    "\n",
    "\n",
    "data_dict = make_data()\n",
    "for key, val in data_dict.items():  # Map data_dict to variables in namespace\n",
    "    exec(key + \"=val\")\n",
    "\n",
    "# Display dataset visually using matplotlib\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=60)\n",
    "true_errors = np.where(true_labels != labels)[0]\n",
    "\n",
    "for i in true_errors:\n",
    "    plt.plot(\n",
    "        data[i][0],\n",
    "        data[i][1],\n",
    "        \"o\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=\"red\",\n",
    "        markersize=14,\n",
    "        markeredgewidth=2.5,\n",
    "        alpha=0.3\n",
    "    )\n",
    "_ = plt.title(\"A realistic, messy dataset with 4 classes.\", fontsize=25)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!-- This cell is for the collapsible block in the doc site -->\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM6E7tNS9pZn"
   },
   "source": [
    "The figure above represents a toy dataset we'll use to demonstrate various cleanlab functionality. In this data, the features *X* are 2-dimensional and examples are colored according to their *given* label above. \n",
    "\n",
    "However, [like many real-world datasets](https://labelerrors.com/), the given label happens to be incorrect for some of the examples (**circled in red**) in this dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmUd-5tljruT"
   },
   "source": [
    "# **Workflow 1:** Use CleanLearning() for everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "AaHC5MRKjruT",
    "outputId": "10ce412e-b3bd-4c71-d34b-6f238d7f7fba"
   },
   "outputs": [],
   "source": [
    "yourFavoriteModel = LogisticRegression(verbose=0, random_state=SEED)\n",
    "\n",
    "# CleanLearning: Machine Learning with cleaned data (given messy, real-world data)\n",
    "cl = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\n",
    "\n",
    "# Fit model to messy, real-world data, automatically training on cleaned data.\n",
    "_ = cl.fit(data, labels)\n",
    "\n",
    "# See the label quality for every example, which data has issues, and more.\n",
    "cl.get_label_issues().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78udGSU6jruT"
   },
   "source": [
    "## Clean Learning = Machine Learning with Cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wy27rvyhjruU",
    "outputId": "9ee09a2f-a88d-4da7-bd3f-640167482420"
   },
   "outputs": [],
   "source": [
    "# For comparison, this is how you would have trained your model normally (without Cleanlab)\n",
    "yourFavoriteModel = LogisticRegression(verbose=0, random_state=SEED)\n",
    "yourFavoriteModel.fit(data, labels)\n",
    "print(\n",
    "    f\"Accuracy using yourFavoriteModel: {yourFavoriteModel.score(test_data, test_labels):.0%}\"\n",
    ")\n",
    "\n",
    "# But CleanLearning can do anything yourFavoriteModel can do, but enhanced.\n",
    "# For example, CleanLearning gives you predictions (just like yourFavoriteModel)\n",
    "# but the magic is that CleanLearning was trained as if your data did not have label errors.\n",
    "print(\n",
    "    f\"Accuracy using yourFavoriteModel (+ CleanLearning): {cl.score(test_data, test_labels):.0%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtEh09G7764o"
   },
   "source": [
    "Note! *Accuracy* refers to the accuracy with respect to the *true* error-free labels of a test set., i.e. what we actually care about in practice because that's what real-world model performance is based on. If you don't have a clean test set, you can use cleanlab to make one :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b8O6_J2jruU"
   },
   "source": [
    "## **Workflow 2:** Use CleanLearning to find_label_issues in one line of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Db8YHnyVjruU",
    "outputId": "69cee9c8-273f-4db6-a72a-723ccdcb3d76"
   },
   "outputs": [],
   "source": [
    "# One line of code. Literally.\n",
    "issues = CleanLearning(yourFavoriteModel, seed=SEED).find_label_issues(data, labels)\n",
    "\n",
    "issues.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OOsvMoMjruU"
   },
   "source": [
    "### Visualize the twenty examples with lowest label quality to see if Cleanlab works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "iJqAHuS2jruV",
    "outputId": "0c608e6a-bb3c-49b9-f0a5-8c61a06180a7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=60)\n",
    "for i in issues[\"label_quality\"].argsort()[:20]:\n",
    "    plt.plot(\n",
    "        data[i][0],\n",
    "        data[i][1],\n",
    "        \"o\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=\"red\",\n",
    "        markersize=14,\n",
    "        markeredgewidth=2.5,\n",
    "    )\n",
    "_ = plt.suptitle(\"The 20 lowest label quality examples\\nerrors found automatically by cleanlab\", fontsize=25)\n",
    "_ = plt.title(\"The 20 lowest label quality examples\\nerrors found automatically by cleanlab\", fontsize=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdtPREswG2fe"
   },
   "source": [
    "Once you have already computed the label issues, you can pass them into `fit()` and it will run much faster (skips label-issue identification step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcPTZ_JJG3Cx",
    "outputId": "207b15a4-4e13-43ef-d81b-f288e97a1224"
   },
   "outputs": [],
   "source": [
    "_ = cl.fit(data, labels, label_issues=issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYFkRMk-jruV"
   },
   "source": [
    "## **Workflow 3:** Use cleanlab to find dataset-level and class-level issues\n",
    "\n",
    "- Did you notice that the yellow and seafoam green class above are overlapping?\n",
    "- How can a model learn what's ground truth inside the yellow distribution?\n",
    "- If these two classes were merged, the model can learn more accurately from 3 classes (versus 4).\n",
    "\n",
    "cleanlab automatically finds data-set level issues like this, in one line of code. Check this out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "0lonvOYvjruV",
    "outputId": "dac35189-d1f7-400e-ce7a-d5fe3aff4c1f"
   },
   "outputs": [],
   "source": [
    "cleanlab.dataset.find_overlapping_classes(\n",
    "    labels=labels,\n",
    "    # confident_joint is computed at cl.fit(). Contains magic cleanlab stats.\n",
    "    confident_joint=cl.confident_joint,\n",
    "    class_names=class_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXkMIKlGjruV"
   },
   "source": [
    "Do the results surprise you? Did you expect the purple and seafoam green to also have so much overlap?\n",
    "\n",
    "There are two things happening here:\n",
    "\n",
    "1. The green distribution has huge variance and its difficult to tell if its a label error or if the classes are simply overlapping distributions. Cleanlab actually handles this case and you can read the theory behind cleanlab for overlapping classes here: https://arxiv.org/abs/1705.01936\n",
    "2. A ton of examples which actually belong to the purple class have been incorrectly labeled as the green class in our dataset. Data can be so sneaky!\n",
    "\n",
    "### Now let's see what happens if we merge the top two classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MfqTCa3kjruV",
    "outputId": "ea996c3a-9456-46c0-adf3-4993e17ccbac"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"[Original classes] Accuracy of yourFavoriteModel: {yourFavoriteModel.score(test_data, test_labels):.0%}\"\n",
    ")\n",
    "\n",
    "merged_labels, merged_test_labels = np.array(labels), np.array(test_labels)\n",
    "\n",
    "# Merge classes: map all yellow-labeled examples to seafoam green\n",
    "merged_labels[merged_labels == 3] = 2\n",
    "merged_test_labels[merged_test_labels == 3] = 2\n",
    "\n",
    "# Re-run our comparison. Re-run your model on the newly labeled dataset.\n",
    "yourFavoriteModel2 = LogisticRegression(verbose=0, random_state=SEED)\n",
    "yourFavoriteModel2.fit(data, merged_labels)\n",
    "print(\n",
    "    f\"[Modified classes] Accuracy of yourFavoriteModel: {yourFavoriteModel2.score(test_data, merged_test_labels):.0%}\"\n",
    ")\n",
    "\n",
    "# Re-run CleanLearning as well.\n",
    "yourFavoriteModel3 = LogisticRegression(verbose=0, random_state=SEED)\n",
    "cl2 = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\n",
    "cl2.fit(data, merged_labels)\n",
    "print(\n",
    "    f\"[Modified classes] Accuracy of yourFavoriteModel (+ CleanLearning): {cl2.score(test_data, merged_test_labels):.0%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi53hnRxjruW"
   },
   "source": [
    "While on one hand that's a huge improvement, it's important to remember that choosing among three classes is an easier task than choosing among four classes, so it's not fair to directly compare these numbers.\n",
    "\n",
    "Instead, the big takeaway is...\n",
    "if you get to choose your classes, combining overlapping classes can make the learning task easier for your model. But if you have lots of classes, how do you know which ones to merge?? That's when you use `cleanlab.dataset.find_overlapping_classes`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxI7bgn8L_1K"
   },
   "source": [
    "## **Workflow 4:** ML with noisy labels and random train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ43QfbrNk0K"
   },
   "source": [
    "If your test and training data were randomly split (IID), then be aware that your test labels are likely noisy too! It is thus important to fix label issues in them before we can trust measures like test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZtWAYXqMAPL",
    "outputId": "8b3cef5e-a82d-4027-8acf-46b61de8e479"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit your model on noisily labeled train data\n",
    "yourFavoriteModel = LogisticRegression(verbose=0, random_state=SEED)\n",
    "yourFavoriteModel.fit(data, labels)\n",
    "\n",
    "# Get predicted probabilities for test data (these are out-of-sample)\n",
    "my_test_pred_probs = yourFavoriteModel.predict_proba(test_data)\n",
    "my_test_preds = my_test_pred_probs.argmax(axis=1)  # predicted labels\n",
    "\n",
    "# Find label issues in the test data\n",
    "issues_test = CleanLearning(yourFavoriteModel, seed=SEED).find_label_issues(labels=noisy_test_labels,\n",
    "                                                                            pred_probs=my_test_pred_probs)\n",
    "\n",
    "# You should inspect issues_test and fix issues to ensure high-quality test data labels. \n",
    "corrected_test_labels = test_labels  # Here we'll pretend you have done this perfectly :)\n",
    "\n",
    "# Fit more robust version of model on noisily labeled training data\n",
    "cl = CleanLearning(yourFavoriteModel, seed=SEED).fit(data, labels)\n",
    "cl_test_preds = cl.predict(test_data)\n",
    "\n",
    "print(\n",
    "    f\"Noisy Accuracy (on given test labels) using yourFavoriteModel: {accuracy_score(noisy_test_labels, my_test_preds):.0%}\"\n",
    ")\n",
    "print(\n",
    "    f\"Noisy Accuracy (on given test labels) using yourFavoriteModel (+ CleanLearning): {accuracy_score(noisy_test_labels, cl_test_preds):.0%}\"\n",
    ")\n",
    "print(\n",
    "    f\"Actual Accuracy (on corrected test labels) using yourFavoriteModel: {accuracy_score(corrected_test_labels, my_test_preds):.0%}\"\n",
    ")\n",
    "print(\n",
    "    f\"Actual Accuracy (on corrected test labels) using yourFavoriteModel (+ CleanLearning): {accuracy_score(corrected_test_labels, cl_test_preds):.0%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GluE5XAAjruW"
   },
   "source": [
    "## **Workflow 5:** Scoring the quality of each individual label\n",
    "\n",
    "cleanlab can analyze every label in a dataset and provide a numerical score gauging its overall quality. Low-quality labels indicate examples that should be more closely inspected, perhaps because their given label is incorrect, or simply because they represent an ambiguous edge-case that's worth a second look.\n",
    "\n",
    "This score can be fairly compared across datasets or across versions of a dataset to track overall dataset quality (a.k.a. *dataset health*) over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rXP3ZPWjruW",
    "outputId": "1c5489a6-d004-4a2e-a022-1bcb50c5f616"
   },
   "outputs": [],
   "source": [
    "# One line of code.\n",
    "health = cleanlab.dataset.overall_label_health_score(\n",
    "    labels, confident_joint=cl.confident_joint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M85Fta_bjruW"
   },
   "source": [
    "### Because we know the truth labels (we created this dataset), we can compare with ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iRPe8KXjruW",
    "outputId": "89ea81e3-2c55-4fff-b5c5-dc47f8a3d970"
   },
   "outputs": [],
   "source": [
    "label_acc = sum(labels != true_labels) / len(labels)\n",
    "print(f\"Percentage of label issues guessed by cleanlab {1 - health:.0%}\")\n",
    "print(\n",
    "    f\"Percentage of (ground truth) label errors): {label_acc:.0%}\",\n",
    ")\n",
    "\n",
    "offset = (1 - label_acc) - health\n",
    "\n",
    "print(\n",
    "    f\"\\nQuestion: cleanlab seems to be overestimating.\"\n",
    "    f\" How do we account for this {offset:.0%} difference?\"\n",
    ")\n",
    "print(\n",
    "    \"Answer: Data points that fall in between two overlapping distributions are often \"\n",
    "    \"impossible to label and are counted as issues.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hxY5lxJjruW"
   },
   "source": [
    "## **Workflow(s) 6:** Use count, rank, filter modules directly\n",
    "\n",
    "- Using these modules directly is intended for more experienced cleanlab users. But once you understand how they work, you can create numerous powerful workflows.\n",
    "- For these workflows, you **always** need two things:\n",
    "  1.  out-of-sample predicted probabilities (e.g. computed via cross-validation) \n",
    "  2.  labels (can contain label errors and various issues)\n",
    "\n",
    "#### cleanlab can compute out-of-sample  predicted probabilities for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpipUliyjruW",
    "outputId": "001942aa-d27a-4ab9-d709-ff6c9cb2bf03"
   },
   "outputs": [],
   "source": [
    "pred_probs = cleanlab.count.estimate_cv_predicted_probabilities(\n",
    "    data, labels, clf=yourFavoriteModel, seed=SEED\n",
    ")\n",
    "print(f\"pred_probs is a {pred_probs.shape} matrix of predicted probabilites\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftWk9CTrjruW"
   },
   "source": [
    "### Fully characterize label noise (noise matrix, inverse noise matrix, joint, prior of true labels)\n",
    "\n",
    "Now that we have `pred_probs` and `labels`, advanced users can compute everything in `cleanlab.count`.\n",
    "\n",
    "- `py: prob(true_label=k)`\n",
    "  - For all classes K, this is the distribution over the actual true labels (which cleanlab can estimate for you even though you don't have the true labels).\n",
    "- `noise_matrix: p(noisy|true)`\n",
    "  - This describes how errors were introduced into your labels. It's a conditional probability matrix with the probability of flipping from the true class to every other class for the given label.\n",
    "- `inverse noise matrix: p(true|noisy)`\n",
    "  - This tells you the probability, for every class, that the true label is actually a different class.\n",
    "- `confident_joint`\n",
    "  - This is an unnormalized (count-based) estimate of the number of examples in our dataset with each possible (true label, given label) pairing.\n",
    "- `joint: p(true label, noisy label)`\n",
    "  - The joint distribution of noisy (given) and true labels is the most useful of all these statistics. From it, you can compute every other statistic listed above. One entry from this matrix can be interpreted as: \"The proportion of examples in our dataset whose true label is *i* and given label is *j*\".\n",
    "\n",
    "These five tools fully characterize class-conditional label noise in a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLq-3q4xjruX",
    "outputId": "d3d03b06-062b-4802-8cdb-3f26d9aa2252"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    py,\n",
    "    noise_matrix,\n",
    "    inverse_noise_matrix,\n",
    "    confident_joint,\n",
    ") = cleanlab.count.estimate_py_and_noise_matrices_from_probabilities(labels, pred_probs)\n",
    "\n",
    "# Note: you can also combine the above two lines of code into a single line of code like this\n",
    "(\n",
    "    py,\n",
    "    noise_matrix,\n",
    "    inverse_noise_matrix,\n",
    "    confident_joint,\n",
    "    pred_probs,\n",
    ") = cleanlab.count.estimate_py_noise_matrices_and_cv_pred_proba(\n",
    "    data, labels, clf=yourFavoriteModel, seed=SEED\n",
    ")\n",
    "\n",
    "# Get the joint distribution of noisy and true labels from the confident joint\n",
    "# This is the most powerful statistic in machine learning with noisy labels.\n",
    "joint = cleanlab.count.estimate_joint(\n",
    "    labels, pred_probs, confident_joint=confident_joint\n",
    ")\n",
    "cleanlab.internal.util.print_joint_matrix(joint)\n",
    "cleanlab.internal.util.print_noise_matrix(noise_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKEsc-rBBbuW"
   },
   "source": [
    "In some applications, you may have a priori knowledge regarding some of these quantities. In this case, you can pass them directly into cleanlab which may be able to leverage this information to better identify label issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5LHhhuqFbXK"
   },
   "outputs": [],
   "source": [
    "cl3 = cleanlab.classification.CleanLearning(yourFavoriteModel, seed=SEED)\n",
    "_ = cl3.fit(data, labels, noise_matrix=noise_matrix_true)  # CleanLearning with a prioiri known noise_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfeJAGyxFFQN"
   },
   "source": [
    "## **Workflow 7:** Get indices of examples with label issues rank-ordered by label quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7w8F8ezBcet",
    "outputId": "54633a21-ac5f-4682-e552-6fb535c25431"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Get out of sample predicted probabilities via cross-validation.\n",
    "# Here we demonstrate the use of sklearn cross_val_predict as another option to get cross-validated predicted probabilities\n",
    "cv_pred_probs = cross_val_predict(\n",
    "    estimator=yourFavoriteModel, X=data, y=labels, cv=3, method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "# Find label issues\n",
    "label_issues_indices = cleanlab.filter.find_label_issues(\n",
    "    labels=labels,\n",
    "    pred_probs=cv_pred_probs,\n",
    "    filter_by=\"both\", # 5 available filter_by options\n",
    "    return_indices_ranked_by=\"self_confidence\",  # 3 available label quality scoring options for rank ordering\n",
    "    rank_by_kwargs={\n",
    "        \"adjust_pred_probs\": True\n",
    "    },  # adjust predicted probabilities (see docstring for more details)\n",
    ")\n",
    "\n",
    "# Return dataset indices of examples with label issues\n",
    "label_issues_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-ANXupQJPH8"
   },
   "source": [
    "\n",
    "### Again, we can visualize the twenty examples with lowest label quality to see if Cleanlab works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "WETRL74tE_sU",
    "outputId": "86a8a5fc-45e5-4cf5-c26f-2cbbaa5abe1b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=60)\n",
    "for i in label_issues_indices[:20]: # top 20 from the list of indices\n",
    "    plt.plot(\n",
    "        data[i][0],\n",
    "        data[i][1],\n",
    "        \"o\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=\"red\",\n",
    "        markersize=14,\n",
    "        markeredgewidth=2.5,\n",
    "    )\n",
    "_ = plt.title(\"The 20 lowest label quality examples\", fontsize=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNkStbegYk7y"
   },
   "source": [
    "## **Workflow 8:** Estimate the number of label issues and use the rank-ordered label quality scores to get the indices of label issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uogYRWFYnuu"
   },
   "outputs": [],
   "source": [
    "# Estimate the number of label issues\n",
    "label_issues_count = cleanlab.count.num_label_issues(\n",
    "    labels=labels,\n",
    "    pred_probs=cv_pred_probs\n",
    ")\n",
    "\n",
    "# Get label quality scores\n",
    "label_quality_scores = cleanlab.rank.get_label_quality_scores(\n",
    "    labels=labels,\n",
    "    pred_probs=cv_pred_probs,\n",
    "    method=\"self_confidence\"\n",
    ")\n",
    "\n",
    "# Rank-order by label quality scores and get the top estimated number of label issues\n",
    "label_issues_indices = np.argsort(label_quality_scores)[:label_issues_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFmrvJyHb0rS"
   },
   "source": [
    "TODO:Curtis I think this requires some explanation vs filter and CL find_label_issues, otherwise it seems confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe-nGjdeYu3J"
   },
   "source": [
    "\n",
    "### Again, we can visualize the twenty examples with lowest label quality to see if Cleanlab works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "pG-ljrmcYp9Q",
    "outputId": "a57cb4de-7a08-44bc-9ef0-b39cfeeef1be"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, s=60)\n",
    "for i in label_issues_indices[:20]: # top 20 from the list of indices\n",
    "    plt.plot(\n",
    "        data[i][0],\n",
    "        data[i][1],\n",
    "        \"o\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=\"red\",\n",
    "        markersize=14,\n",
    "        markeredgewidth=2.5,\n",
    "    )\n",
    "_ = plt.title(\"The 20 lowest label quality examples\", fontsize=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRfHlDlEKyRD"
   },
   "source": [
    "## **Workflow 9:** Ensembling label quality scores from multiple predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL3ngCnuLEWd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# 3 models in ensemble\n",
    "model1 = LogisticRegression(penalty=\"l2\", verbose=0, random_state=SEED)\n",
    "model2 = RandomForestClassifier(max_depth=5, random_state=SEED)\n",
    "model3 = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=3, random_state=SEED\n",
    ")\n",
    "\n",
    "# Get cross-validated predicted probabilities from each model\n",
    "cv_pred_probs_1 = cross_val_predict(\n",
    "    estimator=model1, X=data, y=labels, cv=3, method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "cv_pred_probs_2 = cross_val_predict(\n",
    "    estimator=model2, X=data, y=labels, cv=3, method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "cv_pred_probs_3 = cross_val_predict(\n",
    "    estimator=model3, X=data, y=labels, cv=3, method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "# Get ensemble label quality scores\n",
    "pred_probs_list = [\n",
    "    cv_pred_probs_1,\n",
    "    cv_pred_probs_2,\n",
    "    cv_pred_probs_3,\n",
    "]  # list of predicted probabilities from each model\n",
    "\n",
    "label_quality_scores_best = cleanlab.rank.get_label_quality_ensemble_scores(\n",
    "    labels=labels, pred_probs_list=pred_probs_list, verbose=False\n",
    ")\n",
    "\n",
    "# Alternative approach: create single ensemble predictor and get its pred_probs\n",
    "cv_pred_probs_ensemble = (cv_pred_probs_1 + cv_pred_probs_2 + cv_pred_probs_3)/3  # uniform aggregation of predictions\n",
    "\n",
    "# Use this single set of pred_probs to find label issues\n",
    "label_quality_scores_better = cleanlab.rank.get_label_quality_scores(\n",
    "    labels=labels, pred_probs=cv_pred_probs_ensemble\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-ghgvqVcOJa"
   },
   "source": [
    "While ensembling different models' label quality scores (`label_quality_scores_best`) will often be superior to getting label quality scores from a single ensemble predictor (`label_quality_scores_better`), both approaches produce significantly better label quality scores than just using the predictions from a single model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n44rdl7fNNmQ"
   },
   "source": [
    "## **Bonus:** Try different options of filter_by in find_label_issues() and evaluate precision/recall of detecting true label issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Va6tZIHTNQgx",
    "outputId": "afef9bf7-fad9-4dc2-c8f1-f81af035568b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "yourFavoriteModel = LogisticRegression(verbose=0, random_state=SEED)\n",
    "\n",
    "# get cross-validated predicted probabilities\n",
    "# here we demonstrate the use of sklearn cross_val_predict as another option to get cross-validated predicted probabilities\n",
    "cv_pred_probs = cross_val_predict(\n",
    "    estimator=yourFavoriteModel, X=data, y=labels, cv=3, method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "# ground truth label issues to use for evaluating different filter_by options\n",
    "true_label_issues = (true_labels != labels)\n",
    "\n",
    "# find label issues with different filter_by options\n",
    "filter_by_list = [\n",
    "    \"prune_by_noise_rate\",\n",
    "    \"prune_by_class\",\n",
    "    \"both\",\n",
    "    \"confident_learning\",\n",
    "    \"predicted_neq_given\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for filter_by in filter_by_list:\n",
    "\n",
    "    # find label issues\n",
    "    label_issues = cleanlab.filter.find_label_issues(\n",
    "        labels=labels, \n",
    "        pred_probs=cv_pred_probs, \n",
    "        filter_by=filter_by\n",
    "    )\n",
    "    \n",
    "    precision = precision_score(true_label_issues, label_issues)\n",
    "    recall = recall_score(true_label_issues, label_issues)\n",
    "    \n",
    "    result = {\n",
    "        \"filter_by\": filter_by,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# summary of results\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial_cleanlab_2_0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
