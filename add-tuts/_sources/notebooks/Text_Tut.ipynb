{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Cleanlab, TensorFlow, & SciKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will use Cleanlab to find potential label errors in the IMDb movie review dataset. This dataset contains 50,000 labeled text reviews split evenly in the train and test set. Each review is labeled with a binary sentiment polarity label - positive (1) or negative (0). Cleanlab will shortlist *hundreds* of examples that confuses our ML model the most; many of which are potential label errors, edge cases and obscure examples.\n",
    "\n",
    "**Overview of what we'll do in this tutorial:**\n",
    "\n",
    "- Build a simple TensorFlow & Keras neural net and wrap it with SciKeras to make it scikit-learn compatible.\n",
    "\n",
    "- Compute the out-of-sample predicted probabilities, ``pyx``, with cross validation.\n",
    "\n",
    "- Generate a list of potential label errors with Cleanlab's ``get_noise_indices``.\n",
    "\n",
    "- Build and train aa robust model with Cleanlab's ``LearningWithNoisyLabels`` wrapper. \n",
    "\n",
    "**Data:** https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Install the required dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``%%capture`` is a magic function to hide the cell's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install cleanlab sklearn pandas tensorflow tensorflow-datasets scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Load the ACL's IMDb dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the full dataset from TensorFlow Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_full_ds = tfds.load(name='imdb_reviews', split=('train+test'), batch_size=-1, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into two numpy arrays:\n",
    "1. ``raw_full_texts`` for the movie reviews in text format, and\n",
    "2. ``full_labels`` for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_full_texts, full_labels = tfds.as_numpy(raw_full_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Preprocess the text data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that can standardize the text data in three steps:\n",
    "1. Convert it to lower case\n",
    "2. Remove the HTML break tags, ``<br />``\n",
    "3. Remove any punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html, f'[{re.escape(string.punctuation)}]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ``TextVectorization`` layer that can standardize (by running the ``custom_standardization`` function we've just defined above), tokenize and vectorize our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting ``vectorize_layer`` on our text data creates a mapping of each token to an integer. After that, we can vectorize our text data with the adapted ``vectorize_layer``. Finally, we'll also convert our text data into a numpy array as required by Cleanlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(raw_full_texts)\n",
    "\n",
    "full_texts = vectorize_layer(raw_full_texts)\n",
    "\n",
    "full_texts = full_texts.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Build a classifcation model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build and compile a simple neural network with TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
    "  layers.Embedding(max_features + 1, 16),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Wrap with SciKeras for scikit-learn compatibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of Cleanlab's feature requires scikit-learn compatibility, we will need to adapt the above TensorFlow & Keras neural net accordingly. SciKeras is a convenient package that helps with this, read more about it here: https://www.adriangb.com/scikeras/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(model, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Compute the out-of-sample predicted probabilities with cross validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the entire dataset on the model used to compute the out-of-sample predicted probabilities, ``pyx``, with cross validation. This model will not be used for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 22s 13ms/step - loss: 0.5856 - binary_accuracy: 0.6149\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3889 - binary_accuracy: 0.8289\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.3167 - binary_accuracy: 0.8658\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.2806 - binary_accuracy: 0.8832\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.2582 - binary_accuracy: 0.8929\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.2420 - binary_accuracy: 0.9014\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.2298 - binary_accuracy: 0.9075\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.2199 - binary_accuracy: 0.9114\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.2116 - binary_accuracy: 0.9144\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2051 - binary_accuracy: 0.9169\n"
     ]
    }
   ],
   "source": [
    "_ = model.fit(full_texts, full_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the out-of-sample predicted probabilities, ``pyx``, with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "pyx = cross_val_predict(model, full_texts, full_labels, cv=3, method='predict_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Run Cleanlab's to find potential label errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanlab has a ``get_noise_indices`` function to generate a list of potential label errors. Setting ``sorted_index_method=\"prob_given_label\"`` returns the indices of all the most likely label errors, sorted by the most suspicious example first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.pruning import get_noise_indices\n",
    "\n",
    "ordered_label_errors = get_noise_indices(\n",
    "    s=full_labels,\n",
    "    psx=pyx,\n",
    "    sorted_index_method=\"prob_given_label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Review some of the highest potential label errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanlab found 510 potential label errors. Here are the indices of the top 10 most likely ones: \n",
      " [44582 10404 30151 29182 37856 22370 16633 43777  2468 13853]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cleanlab found {len(ordered_label_errors)} potential label errors. Here are the indices of the top 10 most likely ones: \\n {ordered_label_errors[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change pandas display max column width to ``None`` and define a new function, ``print_as_df``, that can print any example from the raw dataset with just its index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def print_as_df(index):\n",
    "    return pd.DataFrame({'texts': raw_full_texts[index], 'labels': full_labels[index]}, [index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review labeled as positive (1), but should be negative (0). Here are some review snippets:\n",
    "\n",
    "> - \"...incredibly **awful** score...\"\n",
    ">\n",
    "> - \"...**worst** Foley work ever done.\"\n",
    ">\n",
    "> - \"...script is **incomprehensible**...\"\n",
    ">\n",
    "> - \"...editing is just **bizarre**.\"\n",
    ">\n",
    "> - \"...**atrocious** pan and scan...\"\n",
    ">\n",
    "> - \"...**incoherent mess**...\"\n",
    ">\n",
    "> - \"...**amateur** directing there.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44582</th>\n",
       "      <td>b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.&lt;br /&gt;&lt;br /&gt;The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.&lt;br /&gt;&lt;br /&gt;The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.&lt;br /&gt;&lt;br /&gt;The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) &lt;br /&gt;&lt;br /&gt;This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.&lt;br /&gt;&lt;br /&gt;Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.&lt;br /&gt;&lt;br /&gt;Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            texts  \\\n",
       "44582  b'This movie is stuffed full of stock Horror movie goodies: chained lunatics, pre-meditated murder, a mad (vaguely lesbian) female scientist with an even madder father who wears a mask because of his horrible disfigurement, poisoning, spooky castles, werewolves (male and female), adultery, slain lovers, Tibetan mystics, the half-man/half-plant victim of some unnamed experiment, grave robbing, mind control, walled up bodies, a car crash on a lonely road, electrocution, knights in armour - the lot, all topped off with an incredibly awful score and some of the worst Foley work ever done.<br /><br />The script is incomprehensible (even by badly dubbed Spanish Horror movie standards) and some of the editing is just bizarre. In one scene where the lead female evil scientist goes to visit our heroine in her bedroom for one of the badly dubbed: \"That is fantastical. I do not understand. Explain to me again how this is...\" exposition scenes that litter this movie, there is a sudden hand held cutaway of the girl\\'s thighs as she gets out of bed for no apparent reason at all other than to cover a cut in the bad scientist\\'s \"Mwahaha! All your werewolfs belong mine!\" speech. Though why they went to the bother I don\\'t know because there are plenty of other jarring jump cuts all over the place - even allowing for the atrocious pan and scan of the print I saw.<br /><br />The Director was, according to one interview with the star, drunk for most of the shoot and the film looks like it. It is an incoherent mess. It\\'s made even more incoherent by the inclusion of werewolf rampage footage from a different film The Mark of the Wolf Man (made 4 years earlier, featuring the same actor but playing the part with more aggression and with a different shirt and make up - IS there a word in Spanish for \"Continuity\"?) and more padding of another actor in the wolfman get-up ambling about in long shot.<br /><br />The music is incredibly bad varying almost at random from full orchestral creepy house music, to bosannova, to the longest piano and gong duet ever recorded. (Thinking about it, it might not have been a duet. It might have been a solo. The piano part was so simple it could have been picked out with one hand while the player whacked away at the gong with the other.) <br /><br />This is one of the most bewilderedly trance-state inducing bad movies of the year so far for me. Enjoy.<br /><br />Favourite line: \"Ilona! This madness and perversity will turn against you!\" How true.<br /><br />Favourite shot: The lover, discovering his girlfriend slain, dropping the candle in a cartoon-like demonstration of surprise. Rank amateur directing there.'   \n",
       "\n",
       "       labels  \n",
       "44582       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(44582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review labeled as positive (1), but should be negative (0). Here are some review snippets:\n",
    "\n",
    "> - \"...film seems **cheap**.\"\n",
    ">\n",
    "> - \"...unbelievably **bad**...\"\n",
    ">\n",
    "> - \"...cinematography is **badly** lit...\"\n",
    ">\n",
    "> - \"...everything looking **grainy** and **ugly**.\"\n",
    ">\n",
    "> - \"...sound is so **terrible**...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10404</th>\n",
       "      <td>b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    texts  \\\n",
       "10404  b'This low-budget erotic thriller that has some good points, but a lot more bad one. The plot revolves around a female lawyer trying to clear her lover who is accused of murdering his wife. Being a soft-core film, that entails her going undercover at a strip club and having sex with possible suspects. As plots go for this type of genre, not to bad. The script is okay, and the story makes enough sense for someone up at 2 AM watching this not to notice too many plot holes. But everything else in the film seems cheap. The lead actors aren\\'t that bad, but pretty much all the supporting ones are unbelievably bad (one girl seems like she is drunk and/or high). The cinematography is badly lit, with everything looking grainy and ugly. The sound is so terrible that you can barely hear what people are saying. The worst thing in this movie is the reason you\\'re watching it-the sex. The reason people watch these things is for hot sex scenes featuring really hot girls in Red Shoe Diary situations. The sex scenes aren\\'t hot they\\'re sleazy, shot in that porno style where everything is just a master shot of two people going at it. The woman also look like they are refuges from a porn shoot. I\\'m not trying to be rude or mean here, but they all have that breast implants and a burned out/weathered look. Even the title, \"Deviant Obsession\", sounds like a Hardcore flick. Not that I don\\'t have anything against porn - in fact I love it. But I want my soft-core and my hard-core separate. What ever happened to actresses like Shannon Tweed, Jacqueline Lovell, Shannon Whirry and Kim Dawson? Women that could act and who would totally arouse you? And what happened to B erotic thrillers like Body Chemistry, Nighteyes and even Stripped to Kill. Sure, none of these where masterpieces, but at least they felt like movies. Plus, they were pushing the envelope, going beyond Hollywood\\'s relatively prude stance on sex, sexual obsessions and perversions. Now they just make hard-core films without the hard-core sex.'   \n",
       "\n",
       "       labels  \n",
       "10404       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(10404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review labeled as positive (1), but should be negative (0). Here are some review snippets:\n",
    "\n",
    "> - \"...hard to imagine a **boring** shark movie...\"\n",
    ">\n",
    "> - \"**Poor focus** in some scenes made the production seems **amateurish**.\"\n",
    ">\n",
    "> - \"...**do nothing** to take advantage of...\"\n",
    ">\n",
    "> - \"...**far too few** scenes of any depth or variety.\"\n",
    ">\n",
    "> - \"...just **look flat**...no contrast of depth...\"\n",
    ">\n",
    "> - \"...**introspective** and **dull**...constant **disappointment**.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30151</th>\n",
       "      <td>b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      texts  \\\n",
       "30151  b'Like the gentle giants that make up the latter half of this film\\'s title, Michael Oblowitz\\'s latest production has grace, but it\\'s also slow and ponderous. The producer\\'s last outing, \"Mosquitoman-3D\" had the same problem. It\\'s hard to imagine a boring shark movie, but they somehow managed it. The only draw for Hammerhead: Shark Frenzy was it\\'s passable animatronix, which is always fun when dealing with wondrous worlds beneath the ocean\\'s surface. But even that was only passable. Poor focus in some scenes made the production seems amateurish. With Dolphins and Whales, the technology is all but wasted. Cloudy scenes and too many close-ups of the film\\'s giant subjects do nothing to take advantage of IMAX\\'s stunning 3D capabilities. There are far too few scenes of any depth or variety. Close-ups of these awesome creatures just look flat and there is often only one creature in the cameras field, so there is no contrast of depth. Michael Oblowitz is trying to follow in his father\\'s footsteps, but when you\\'ve got Shark-Week on cable, his introspective and dull treatment of his subjects is a constant disappointment.'   \n",
       "\n",
       "       labels  \n",
       "30151       1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_as_df(30151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanlab has shortlisted the most likely label errors to speed up your data cleaning process. With this list, you can decide whether to fix label errors, augment edge cases or remove obscure examples. \n",
    "\n",
    "These human-in-the-loop processes may be time-consuming, so if you'd like Cleanlab to automatically remove these noisy examples and train a model directly on the partially mislabeled dataset, you're in luck! Cleanlab provides a ``LearningWithNoisyLabels`` wrapper to do precisely this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Adapt with Cleanlab's wrapper and train a robust model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we will load the dataset, but this time, we will load the train and test set separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = tfds.load(name='imdb_reviews', split='train', batch_size=-1, as_supervised=True)\n",
    "raw_test_ds = tfds.load(name='imdb_reviews', split='test', batch_size=-1, as_supervised=True)\n",
    "\n",
    "raw_train_texts, train_labels = tfds.as_numpy(raw_train_ds)\n",
    "raw_test_texts, test_labels = tfds.as_numpy(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same ``vectorize_layer`` as before, but we will reset its state and adapt it only on the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.reset_state() \n",
    "\n",
    "vectorize_layer.adapt(raw_train_texts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the text data in the train and test sets, then convert them into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = vectorize_layer(raw_train_texts) \n",
    "test_texts = vectorize_layer(raw_test_texts)\n",
    "\n",
    "train_texts = train_texts.numpy()\n",
    "test_texts = test_texts.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ``clone`` function to construct a new unfitted model then wrap it with Cleanlab's ``LearningWithNoisyLabels`` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "\n",
    "model = clone(model)\n",
    "\n",
    "lnl = LearningWithNoisyLabels(clf=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the wrapped model, ``lnl``, on the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 11ms/step - loss: 0.8739 - binary_accuracy: 0.6055\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.6611 - binary_accuracy: 0.6918\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.5503 - binary_accuracy: 0.7412\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4792 - binary_accuracy: 0.7757\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.4286 - binary_accuracy: 0.8007\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.3916 - binary_accuracy: 0.8195\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3601 - binary_accuracy: 0.8386\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3337 - binary_accuracy: 0.8532\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3138 - binary_accuracy: 0.8643\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.2939 - binary_accuracy: 0.8759\n",
      "157/157 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.8800 - binary_accuracy: 0.6046\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.6658 - binary_accuracy: 0.6884\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.5549 - binary_accuracy: 0.7390\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4831 - binary_accuracy: 0.7748\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4336 - binary_accuracy: 0.7997\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.3954 - binary_accuracy: 0.8216\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.3642 - binary_accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.3402 - binary_accuracy: 0.8523\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.3164 - binary_accuracy: 0.8655\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.2980 - binary_accuracy: 0.8737\n",
      "157/157 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.8838 - binary_accuracy: 0.6029\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.6676 - binary_accuracy: 0.6898\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5513 - binary_accuracy: 0.7388\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.4831 - binary_accuracy: 0.7734\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.4339 - binary_accuracy: 0.8010\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3940 - binary_accuracy: 0.8210\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3647 - binary_accuracy: 0.8360\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.3397 - binary_accuracy: 0.8511\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3162 - binary_accuracy: 0.8658\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.2977 - binary_accuracy: 0.8734\n",
      "157/157 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 11ms/step - loss: 0.8802 - binary_accuracy: 0.6005\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.6667 - binary_accuracy: 0.6902\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.5549 - binary_accuracy: 0.7406\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.4844 - binary_accuracy: 0.7718\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4343 - binary_accuracy: 0.8001\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.3960 - binary_accuracy: 0.8218\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3645 - binary_accuracy: 0.8376\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 12ms/step - loss: 0.3401 - binary_accuracy: 0.8506\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3174 - binary_accuracy: 0.8646\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.2980 - binary_accuracy: 0.8738\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 11ms/step - loss: 0.8815 - binary_accuracy: 0.6001\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.6717 - binary_accuracy: 0.6848\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.5573 - binary_accuracy: 0.7363\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4863 - binary_accuracy: 0.7716\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4370 - binary_accuracy: 0.7933\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3988 - binary_accuracy: 0.8172\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3676 - binary_accuracy: 0.8343\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3423 - binary_accuracy: 0.8484\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3210 - binary_accuracy: 0.8621\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3011 - binary_accuracy: 0.8727\n",
      "157/157 [==============================] - 0s 3ms/step\n",
      "Epoch 1/10\n",
      "742/742 [==============================] - 11s 14ms/step - loss: 0.6741 - binary_accuracy: 0.6870\n",
      "Epoch 2/10\n",
      "742/742 [==============================] - 11s 14ms/step - loss: 0.4379 - binary_accuracy: 0.7924\n",
      "Epoch 3/10\n",
      "742/742 [==============================] - 11s 14ms/step - loss: 0.3503 - binary_accuracy: 0.8361\n",
      "Epoch 4/10\n",
      "742/742 [==============================] - 11s 14ms/step - loss: 0.2949 - binary_accuracy: 0.8646\n",
      "Epoch 5/10\n",
      "742/742 [==============================] - 11s 14ms/step - loss: 0.2516 - binary_accuracy: 0.8862\n",
      "Epoch 6/10\n",
      "742/742 [==============================] - 11s 15ms/step - loss: 0.2198 - binary_accuracy: 0.9048\n",
      "Epoch 7/10\n",
      "742/742 [==============================] - 11s 15ms/step - loss: 0.1947 - binary_accuracy: 0.9157\n",
      "Epoch 8/10\n",
      "742/742 [==============================] - 11s 14ms/step - loss: 0.1726 - binary_accuracy: 0.9276\n",
      "Epoch 9/10\n",
      "742/742 [==============================] - 11s 15ms/step - loss: 0.1546 - binary_accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "742/742 [==============================] - 11s 15ms/step - loss: 0.1394 - binary_accuracy: 0.9452\n"
     ]
    }
   ],
   "source": [
    "_ = lnl.fit(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Evaluate the robust model's performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83128"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred_labels = lnl.predict(test_texts)\n",
    "accuracy_score(test_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What's next?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation on completing this tutorial! Check out our following tutorial on using Cleanlab for tabular data classification!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text x TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
